1. Non-english text when usually used for tokenization will use up more tokens, so for the same context window, non-english tokens will capture less of a meaning making it difficult to learn
2. The same way, if there's lots of space in the text (coding indentation), that uses a lots of unnecessary tokens (basically ones that doesn't convey any meaning but occupies tokens!)
3. Increasing number of tokens can reduce the breaking of text i.e the same text that uses 300 tokens for gpt2 (50k tokens) might use around 200 tokens when used gpt4 tokenizer (as it has 100k tokens). This will reduce the inference cost as we are using fewer tokens to represent the text and conveys meaning better with a fixed context window (as CW of 512 for GPT2 might have a lot of waste tokens but with gpt4, there can be fewer). But the caveat is, increasing this will increase the embedding lookup table (thus the memory) and also at the output decoding the softmax layer over 50k vs 5Mn tokens makes a lot of difference in terms of output quality.
4. All the characters text (dubbed as code points) fall into the unicode ones, there's around 150K. But we can't simply use this and get away with tokenization because the Unicode is live (changes frequently) and also a huge corpus with not very meaning (i.e it's just character level)
5. UTF-8 is the most common encoding scheme where each code point is converted to a variable (1-4 byte) number (refer more blogs eg: https://tonsky.me/blog/unicode/, https://www.reedbeta.com/blog/programmers-intro-to-unicode/)
6. Just using UTF-8 will have a small vocab size  of 256 thus a smaller lookup table, but because of this the document will occupy lot more tokens and due to limited context size, the attention might not use all the information needed to predict properly!
7. BPE (Byte-pair encoding) -> Iteratively look at the bigrams and replace the most frequent one with one token i.e add that bigram to vocabulary!
8. Training Tokenizer (will have it's own training set) compared to training LLM. This BPE training is important because that's what is going to the LLM. So, there's no point to do LLM training if tokenizer training is not good!
9. More Japanese data in tokenization training -> More japanese tokens get's merged during BPE thus the final vocabulary has more Japanese representation -> When the LLM looks at new text, if it's Japanese..as it has more representation, this text will be broken to fewer tokens, thus fits in limited context window which gives good predictions.
10. AI Safety issues for tokenization. SolkdGoldmagicKarp is present in tokenization data, so this token gets a row in the embedding table. But because this token is not present in the training data, that embedding row won't get updated and thus stays random. So, during inference, when we give this token, the model receives a random embedding and thus predicts unexpected outputs!      